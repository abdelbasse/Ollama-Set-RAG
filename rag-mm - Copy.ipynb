{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a RAG (Retrieval-Augmented Generation) system with ChromaDB for storing and searching vectorized data, we can adapt your example to handle the `.csv` file containing Java snippets. Here's a step-by-step explanation and an example in Python:\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Load the Dataset**: Read the `javaSnippets.csv` file.\n",
    "2. **Embed Text**: Use a language model to embed the Java snippets into vector representations.\n",
    "3. **Store in ChromaDB**: Save the embeddings into ChromaDB for retrieval.\n",
    "4. **Search for Snippets**: Perform similarity searches on the database using vectorized queries.\n",
    "\n",
    "### 1. **Install Required Libraries**\n",
    "\n",
    "Before starting, install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 2. **Load and Inspect Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "      id                                            snippet language  \\\n",
      "0  88608  /*\\r\\n * Copyright 2014 The Netty Project\\r\\n ...     Java   \n",
      "1  88609   * with the License. You may obtain a copy of ...     Java   \n",
      "2  88610   * distributed under the License is distribute...     Java   \n",
      "3  88611  \\r\\npackage io.netty.resolver.dns;\\r\\n\\r\\nimpo...     Java   \n",
      "4  88612  \\r\\nimport java.net.InetSocketAddress;\\r\\nimpo...     Java   \n",
      "\n",
      "                                      repo_file_name  \\\n",
      "0  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
      "1  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
      "2  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
      "3  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
      "4  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
      "\n",
      "                  github_repo_url     license  \\\n",
      "0  https://github.com/netty/netty  Apache-2.0   \n",
      "1  https://github.com/netty/netty  Apache-2.0   \n",
      "2  https://github.com/netty/netty  Apache-2.0   \n",
      "3  https://github.com/netty/netty  Apache-2.0   \n",
      "4  https://github.com/netty/netty  Apache-2.0   \n",
      "\n",
      "                                    commit_hash  starting_line_number  \\\n",
      "0  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                     0   \n",
      "1  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                     5   \n",
      "2  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                    10   \n",
      "3  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                    15   \n",
      "4  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                    20   \n",
      "\n",
      "   chunk_size  \n",
      "0           5  \n",
      "1           5  \n",
      "2           5  \n",
      "3           5  \n",
      "4           5  \n",
      "Loaded 7541424 code snippets.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = './datasets/snippets/javaSnippets.csv'  # Update with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Ensure the column with code snippets is correctly identified\n",
    "column_name = 'snippet'\n",
    "assert column_name in df.columns, f\"Column '{column_name}' not found in the dataset.\"\n",
    "\n",
    "# Extract snippets\n",
    "snippets = df[column_name].tolist()\n",
    "print(f\"Loaded {len(snippets)} code snippets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To clean your dataset (`javaSnippets.csv`), you can follow a structured process similar to the example you provided. Here's how you can clean the data:\n",
    "\n",
    "### Step-by-Step Data Cleaning\n",
    "1. **Check for Missing Values**:\n",
    "   Identify which columns have missing values and decide whether to drop rows or columns or fill missing values.\n",
    "\n",
    "2. **Remove Unnecessary Characters**:\n",
    "   Since the dataset includes code snippets, it’s essential to clean characters like extra whitespace or unnecessary escape sequences (`\\r\\n`).\n",
    "\n",
    "3. **Check for Duplicates**:\n",
    "   Remove duplicate rows to avoid redundant data.\n",
    "\n",
    "4. **Validate Data Types**:\n",
    "   Ensure columns have correct data types (e.g., `starting_line_number` and `chunk_size` should be integers).\n",
    "\n",
    "5. **Filter Invalid Data**:\n",
    "   Remove rows with invalid or irrelevant data (e.g., empty or overly short snippets).\n",
    "\n",
    "Here's the Python code for cleaning your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[dataframe] in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (2024.12.1)\n",
      "Requirement already satisfied: click>=8.1 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from dask[dataframe]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from dask[dataframe]) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from dask[dataframe]) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from dask[dataframe]) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from dask[dataframe]) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from dask[dataframe]) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from dask[dataframe]) (1.0.0)\n",
      "Requirement already satisfied: pandas>=2.0 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from dask[dataframe]) (2.2.3)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n",
      "  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from click>=8.1->dask[dataframe]) (0.4.6)\n",
      "Collecting pyarrow>=14.0.1 (from dask-expr<1.2,>=1.1->dask[dataframe])\n",
      "  Using cached pyarrow-18.1.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from pandas>=2.0->dask[dataframe]) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from pandas>=2.0->dask[dataframe]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
      "Requirement already satisfied: locket in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.17.0)\n",
      "Downloading dask_expr-1.1.21-py3-none-any.whl (244 kB)\n",
      "Using cached pyarrow-18.1.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "Installing collected packages: pyarrow, dask-expr\n",
      "Successfully installed dask-expr-1.1.21 pyarrow-18.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install dask[dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      0\n",
       "snippet                 0\n",
       "language                0\n",
       "repo_file_name          0\n",
       "github_repo_url         0\n",
       "license                 0\n",
       "commit_hash             0\n",
       "starting_line_number    0\n",
       "chunk_size              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n",
    "# Columns exp and topic_name contain missing values, we will drop rows with missing values in both columns\n",
    "df = df.dropna(subset=[\"snippet\"])\n",
    "df.isna().sum()\n",
    "df = df.dropna(subset=[\"repo_file_name\"])\n",
    "df.isna().sum()\n",
    "df = df.dropna(subset=[\"github_repo_url\"])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "1. **Handling Missing Values**:\n",
    "   - Identifies columns with missing values and drops rows where critical columns (`snippet`, `repo_file_name`, etc.) are empty.\n",
    "   \n",
    "2. **Removing Unnecessary Characters**:\n",
    "   - Replaces `\\r\\n` with `\\n` for better readability of the code snippets.\n",
    "   - Trims leading/trailing whitespace.\n",
    "\n",
    "3. **Removing Duplicates**:\n",
    "   - Removes rows with identical values across all columns.\n",
    "\n",
    "4. **Validating Data Types**:\n",
    "   - Ensures numeric columns like `starting_line_number` and `chunk_size` are properly formatted.\n",
    "\n",
    "5. **Filtering Invalid Snippets**:\n",
    "   - Removes rows with overly short or invalid snippets (e.g., fewer than 10 characters).\n",
    "\n",
    "### Next Steps\n",
    "- Save the cleaned dataset for further processing:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.11-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.2.1-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.11.11-cp312-cp312-win_amd64.whl (437 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Using cached propcache-0.2.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Installing collected packages: xxhash, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 attrs-24.3.0 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 35855.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 429234.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 488487.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 528795.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 448382.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 515201.14 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 282861.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 420532.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 435346.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 923408.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 445723.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 363099.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 442736.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 491746.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 211264.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 307583.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 389678.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 278079.71 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 151073.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 340010.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 901748.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 447569.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 362669.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 281564.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 260194.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 518493.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 447101.51 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 296030.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 707147.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 223557.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 307439.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 153510.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 350304.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 203425.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 250201.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 278465.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 162317.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 241147.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 243400.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 137800.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 35283.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 350263.81 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 550238.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 264659.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 387028.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 216102.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 273356.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 35333.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 296782.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 285072.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 389703.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 333768.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 285055.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 638363.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 635173.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 255926.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 224937.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 102657.65 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 40156.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 110907.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 74211.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 83114.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 44952.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 70594.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 48632.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 16554.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 63242.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 55930.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 216009.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 237958.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 325240.69 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 159751.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 568649.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 385275.71 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 261254.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 147444.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 291651.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 100501.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 40599.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 66245.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 170417.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 799234.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 711514.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 453600.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 274937.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 60703.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 375262.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 268972.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 313522.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 339760.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 289284.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 314691.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 268776.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 84542.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 86091.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 106766.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 97085.65 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 32243.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 146950.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 18848.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 348407.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 265606.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 172856.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 252583.71 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 368697.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 308162.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 354410.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 317950.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 101249.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 111189.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 59801.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 112068.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 253570.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 291489.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 296582.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 112036.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 156870.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 278164.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 417518.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 244382.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 267929.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 315626.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 405180.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 189065.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 89139.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 113817.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 105709.09 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 23440.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 361531.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 235151.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 59737.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 366593.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 506729.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 265054.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 225841.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 433667.71 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 268246.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 303532.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 291390.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 268741.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 93586.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 26625.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 36310.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 62678.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 81870.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 47458.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 63609.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 317092.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 379115.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 326168.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 216753.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 249417.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 637509.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 108413.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 72202.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 66864.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 61089.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 81439.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 84005.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 188746.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 192085.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 114359.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 188462.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 142615.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 98820.18 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 49171.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 193788.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 268068.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 233535.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 281824.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 351308.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 118524.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 60098.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 31922.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 21597.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 78296.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 39007.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 229021.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 249533.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 265701.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 231978.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 392280.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 284716.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 304400.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 84887.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 78042.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 52474.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 14228.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 147891.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 258749.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 300932.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 275814.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 376076.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 259206.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 366798.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 293465.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 83818.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 160328.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 64991.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 36217.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 193147.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 286214.65 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 135330.69 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 59282.95 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 566086.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 247991.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 263033.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 240258.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 231079.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 246638.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 328398.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 273338.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 117948.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 101034.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 183463.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 21133.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 67262.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 56692.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 10581.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 26077.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 135872.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 319895.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 240790.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 261942.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 324152.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 199063.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 287135.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 246428.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 80894.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 118826.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 234530.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 285167.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 421711.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 277492.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 271993.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 370233.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 247887.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 222371.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 52611.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 127348.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 52940.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 23742.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 36982.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 21074.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 194069.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 203449.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 391409.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 227555.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 472336.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 351187.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 524261.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 574278.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 163735.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 176936.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 59473.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 518179.95 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 587923.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 384847.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 289631.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 304741.09 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 372817.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 277406.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 105319.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 47155.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 29613.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 45718.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 163125.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 71473.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 37286.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 46254.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 301659.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 260115.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 249186.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 250491.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 233813.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 763072.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 401660.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 121096.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 78939.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 99595.24 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 155419.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 24387.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 121711.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 77960.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 59008.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 224701.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 243546.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 239795.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 287746.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 231965.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 180963.69 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 129360.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 163239.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 257717.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 382618.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 254453.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 303253.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 139330.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 76095.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 80971.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 168650.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 15713.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 157881.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 268085.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 175248.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 164322.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 174650.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 91813.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 209795.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 114666.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 284643.27 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 212509.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 386329.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 239647.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 266129.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 230699.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 88438.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 24269.95 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 39217.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 54772.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 68977.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 211900.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 144663.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 148868.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 155352.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 233200.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 214592.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 82712.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 144246.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 161092.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 116657.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 291728.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 163439.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 51451.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 29551.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 163576.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 32247.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 156543.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 208962.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 158976.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 124723.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 80036.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 162974.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 150813.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 175132.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 112959.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 146526.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 196449.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 198927.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 188418.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 105990.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 269385.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 35845.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 30148.71 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 63659.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 136402.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 103561.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 222362.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 230826.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 223271.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 183251.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 291097.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 246995.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 89614.03 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 20382.69 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 21132.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 109106.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 295225.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 216944.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 269889.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 235909.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 203841.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 58795.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 201419.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 160412.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 149755.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 265472.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 31084.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 199850.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 189851.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 252969.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 241674.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 237263.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 234827.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 76196.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 92268.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 79605.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 303580.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 276722.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 376867.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 141583.71 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 114701.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 158313.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 121218.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 145401.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 395204.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 287348.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 202800.71 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 242827.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 179736.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 13454.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 283632.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 158149.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 143788.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 618628.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 318624.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 575310.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 454844.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 346714.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 274811.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 218264.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 282562.69 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 238693.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 131838.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 83752.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 139980.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 51517.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 69322.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 207721.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 31419.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 38458.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 36120.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 127218.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 293205.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 245588.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 213231.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 127462.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 38589.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 201951.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 134752.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 213399.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 269525.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 283832.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 120758.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 109399.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 140915.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 51793.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 239358.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 639912.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 258010.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 347619.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 259514.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 46954.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 62168.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 61650.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 157303.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 261757.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 348141.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 69781.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 142181.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 429352.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 85896.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 165908.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 294031.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 84252.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 72159.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 21721.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 26930.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 95646.59 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 199410.65 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 358172.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 402733.09 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 279866.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 200914.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 104741.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 237215.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 158407.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 194327.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 316186.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 104404.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 136331.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 86991.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 96228.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 103960.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 304517.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 214421.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 239054.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 111313.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 131840.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 112607.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 595325.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 235166.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 403441.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 275843.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 352166.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 61154.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 116076.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 41675.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 45790.69 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 61425.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 43234.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 207979.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 279162.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 287868.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 201688.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 222126.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 304254.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 174701.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 109902.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 45714.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 189685.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 297869.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 122157.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 249946.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 129300.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 122714.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 120695.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 51381.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 340446.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 636648.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 140977.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 72021.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 36437.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 313681.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 157332.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 247765.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 165597.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 186819.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 199807.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 633150.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 173586.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 252940.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 315912.27 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 290353.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 260331.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 293105.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 42137.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 167325.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 119406.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 16296.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 30917.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 13224.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 60301.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 245745.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 335606.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 217719.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 392273.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 302951.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 305162.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 375433.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 226831.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 345505.95 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 382489.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 187998.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 310046.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 228989.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 295034.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 129522.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 102162.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 16393.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 18479.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 54826.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 23060.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 235995.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 119957.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 226656.65 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 140552.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 175055.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 233031.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 144044.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 96627.14 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 67086.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 126738.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 155269.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 214644.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 198832.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 205445.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 361868.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 355380.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 57071.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 73494.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 165581.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 349301.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 168025.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 307122.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 14859.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 190534.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 192491.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 236472.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 324380.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 42245.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 129387.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 637587.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 284226.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 244145.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 346350.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 340474.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 245848.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 155116.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 51546.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 31214.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 81938.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 221591.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 118578.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 143377.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 267051.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 287489.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 439138.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 267577.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 245421.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 234969.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 277597.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 187013.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 115528.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 109738.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 64634.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 48583.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 79000.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 153482.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 196673.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 219131.27 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 252574.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 267064.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 387017.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 308101.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 295696.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 255458.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 347645.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 116352.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 191380.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 54886.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 98140.38 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 128683.71 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 51270.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 138544.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 60921.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 16286.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 26934.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 53287.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 181529.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 274108.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 377514.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 290130.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 280289.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 636851.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 74658.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 88706.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 17911.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 19801.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 356062.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 366276.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 210655.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 419296.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 149189.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 153048.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 265856.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 235496.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 421212.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 254359.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 265239.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 251408.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 222468.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 184706.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 164530.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 134790.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 104205.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 106666.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 139003.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 66753.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 80695.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 38733.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 38544.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 41394.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 65306.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 105326.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 639648.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 221276.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 239182.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 246464.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 245412.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 747474.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 201115.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 138388.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 90620.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 99167.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 200335.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 327357.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 249368.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 226338.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 229832.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 71756.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 137993.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 44316.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 379832.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 260575.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 240194.71 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 246853.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 223587.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 183025.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 148779.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 299103.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 121001.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 890794.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 344608.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 209782.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 105581.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 24111.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 57249.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 51544.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 73309.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 93195.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 105936.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 146390.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 131643.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 297230.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 153389.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 234106.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 233076.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 247965.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 65645.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 176406.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 235345.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 636204.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 273749.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 216412.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 270862.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 313119.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 318849.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 176591.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 192472.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 46783.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 78188.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 53310.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 24063.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 73972.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 107447.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 231992.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 234512.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 390953.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 245446.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 239166.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 308643.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 223376.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 273663.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 70313.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 109371.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 76337.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 175901.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 148244.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 194087.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 288716.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 256070.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 436188.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 287608.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 363174.65 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 273180.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 353848.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 255286.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 183549.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 121040.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 60576.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 32979.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 32206.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 75886.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1424/1424 [00:00<00:00, 4545.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Define a function to split the dataframe into smaller chunks\n",
    "def process_in_chunks(df, chunk_size):\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunk = df[i * chunk_size:(i + 1) * chunk_size]\n",
    "        dataset = Dataset.from_pandas(chunk)\n",
    "        dataset.save_to_disk(f\"./datasets/snippets/cleaned_chunk_{i}\")\n",
    "\n",
    "# Process the dataframe in chunks (adjust the chunk size as needed)\n",
    "process_in_chunks(df, chunk_size=10000)  # For example, 10,000 rows per chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 88608,\n",
       " 'snippet': '/*\\r\\n * Copyright 2014 The Netty Project\\r\\n *\\r\\n * The Netty Project licenses this file to you under the Apache License,\\r\\n * version 2.0 (the \"License\"); you may not use this file except in compliance\\r\\n',\n",
       " 'language': 'Java',\n",
       " 'repo_file_name': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       " 'github_repo_url': 'https://github.com/netty/netty',\n",
       " 'license': 'Apache-2.0',\n",
       " 'commit_hash': 'a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n',\n",
       " 'starting_line_number': 0,\n",
       " 'chunk_size': 5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chunk_0 = Dataset.load_from_disk(\"./datasets/snippets/cleaned_chunk_0\")\n",
    "\n",
    "dataset_chunk_0[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now that you have embeddings working fine, it's time to store those embeddings in a vector database like ChromaDB. ChromaDB is a vector database that can store embeddings efficiently and support similarity search, which is useful for tasks like semantic search.\n",
    "\n",
    "Here’s how you can modify your code to store and retrieve embeddings using ChromaDB:\n",
    "\n",
    "### Step 1: Install Required Libraries\n",
    "\n",
    "Make sure you have the required libraries installed:\n",
    "\n",
    "- `chromadb`\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "\n",
    "You can install ChromaDB using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Create Collection**\n",
    "You can now insert documents into the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"JavaCodeSnippets-store-vdb\")\n",
    "\n",
    "# Create or connect to collection\n",
    "collection_name = 'JavaCodeSnippetsDB'\n",
    "\n",
    "# Check if collection exists by comparing names\n",
    "existing_collections = [collection.name for collection in chroma_client.list_collections()]\n",
    "\n",
    "if collection_name not in existing_collections:\n",
    "    chroma_client.create_collection(collection_name)\n",
    "\n",
    "# Get the collection\n",
    "collection = chroma_client.get_collection(collection_name)\n",
    "\n",
    "collection = chroma_client.get_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------\n",
    "chroma_client.delete_collection(collection_name) # Deletes the collection\n",
    "# ------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB is working! Collections found: Collection(name=JavaCodeSnippetsDB)\n"
     ]
    }
   ],
   "source": [
    "if collection:\n",
    "    print(\"ChromaDB is working! Collections found:\", collection)\n",
    "else:\n",
    "    print(\"No collections found. Creating a new collection...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Add Data to the Collection**\n",
    "You can now insert documents into the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directories: ['cleaned_chunk_0', 'cleaned_chunk_1', 'cleaned_chunk_10', 'cleaned_chunk_100', 'cleaned_chunk_101', 'cleaned_chunk_102', 'cleaned_chunk_103', 'cleaned_chunk_104', 'cleaned_chunk_105', 'cleaned_chunk_106', 'cleaned_chunk_107', 'cleaned_chunk_108', 'cleaned_chunk_109', 'cleaned_chunk_11', 'cleaned_chunk_110', 'cleaned_chunk_111', 'cleaned_chunk_112', 'cleaned_chunk_113', 'cleaned_chunk_114', 'cleaned_chunk_115', 'cleaned_chunk_116', 'cleaned_chunk_117', 'cleaned_chunk_118', 'cleaned_chunk_119', 'cleaned_chunk_12', 'cleaned_chunk_120', 'cleaned_chunk_121', 'cleaned_chunk_122', 'cleaned_chunk_123', 'cleaned_chunk_124', 'cleaned_chunk_125', 'cleaned_chunk_126', 'cleaned_chunk_127', 'cleaned_chunk_128', 'cleaned_chunk_129', 'cleaned_chunk_13', 'cleaned_chunk_130', 'cleaned_chunk_131', 'cleaned_chunk_132', 'cleaned_chunk_133', 'cleaned_chunk_134', 'cleaned_chunk_135', 'cleaned_chunk_136', 'cleaned_chunk_137', 'cleaned_chunk_138', 'cleaned_chunk_139', 'cleaned_chunk_14', 'cleaned_chunk_140', 'cleaned_chunk_141', 'cleaned_chunk_142', 'cleaned_chunk_143', 'cleaned_chunk_144', 'cleaned_chunk_145', 'cleaned_chunk_146', 'cleaned_chunk_147', 'cleaned_chunk_148', 'cleaned_chunk_149', 'cleaned_chunk_15', 'cleaned_chunk_150', 'cleaned_chunk_151', 'cleaned_chunk_152', 'cleaned_chunk_153', 'cleaned_chunk_154', 'cleaned_chunk_155', 'cleaned_chunk_156', 'cleaned_chunk_157', 'cleaned_chunk_158', 'cleaned_chunk_159', 'cleaned_chunk_16', 'cleaned_chunk_160', 'cleaned_chunk_161', 'cleaned_chunk_162', 'cleaned_chunk_163', 'cleaned_chunk_164', 'cleaned_chunk_165', 'cleaned_chunk_166', 'cleaned_chunk_167', 'cleaned_chunk_168', 'cleaned_chunk_169', 'cleaned_chunk_17', 'cleaned_chunk_170', 'cleaned_chunk_171', 'cleaned_chunk_172', 'cleaned_chunk_173', 'cleaned_chunk_174', 'cleaned_chunk_175', 'cleaned_chunk_176', 'cleaned_chunk_177', 'cleaned_chunk_178', 'cleaned_chunk_179', 'cleaned_chunk_18', 'cleaned_chunk_180', 'cleaned_chunk_181', 'cleaned_chunk_182', 'cleaned_chunk_183', 'cleaned_chunk_184', 'cleaned_chunk_185', 'cleaned_chunk_186', 'cleaned_chunk_187', 'cleaned_chunk_188', 'cleaned_chunk_189', 'cleaned_chunk_19', 'cleaned_chunk_190', 'cleaned_chunk_191', 'cleaned_chunk_192', 'cleaned_chunk_193', 'cleaned_chunk_194', 'cleaned_chunk_195', 'cleaned_chunk_196', 'cleaned_chunk_197', 'cleaned_chunk_198', 'cleaned_chunk_199', 'cleaned_chunk_2', 'cleaned_chunk_20', 'cleaned_chunk_200', 'cleaned_chunk_201', 'cleaned_chunk_202', 'cleaned_chunk_203', 'cleaned_chunk_204', 'cleaned_chunk_205', 'cleaned_chunk_206', 'cleaned_chunk_207', 'cleaned_chunk_208', 'cleaned_chunk_209', 'cleaned_chunk_21', 'cleaned_chunk_210', 'cleaned_chunk_211', 'cleaned_chunk_212', 'cleaned_chunk_213', 'cleaned_chunk_214', 'cleaned_chunk_215', 'cleaned_chunk_216', 'cleaned_chunk_217', 'cleaned_chunk_218', 'cleaned_chunk_219', 'cleaned_chunk_22', 'cleaned_chunk_220', 'cleaned_chunk_221', 'cleaned_chunk_222', 'cleaned_chunk_223', 'cleaned_chunk_224', 'cleaned_chunk_225', 'cleaned_chunk_226', 'cleaned_chunk_227', 'cleaned_chunk_228', 'cleaned_chunk_229', 'cleaned_chunk_23', 'cleaned_chunk_230', 'cleaned_chunk_231', 'cleaned_chunk_232', 'cleaned_chunk_233', 'cleaned_chunk_234', 'cleaned_chunk_235', 'cleaned_chunk_236', 'cleaned_chunk_237', 'cleaned_chunk_238', 'cleaned_chunk_239', 'cleaned_chunk_24', 'cleaned_chunk_240', 'cleaned_chunk_241', 'cleaned_chunk_242', 'cleaned_chunk_243', 'cleaned_chunk_244', 'cleaned_chunk_245', 'cleaned_chunk_246', 'cleaned_chunk_247', 'cleaned_chunk_248', 'cleaned_chunk_249', 'cleaned_chunk_25', 'cleaned_chunk_250', 'cleaned_chunk_251', 'cleaned_chunk_252', 'cleaned_chunk_253', 'cleaned_chunk_254', 'cleaned_chunk_255', 'cleaned_chunk_256', 'cleaned_chunk_257', 'cleaned_chunk_258', 'cleaned_chunk_259', 'cleaned_chunk_26', 'cleaned_chunk_260', 'cleaned_chunk_261', 'cleaned_chunk_262', 'cleaned_chunk_263', 'cleaned_chunk_264', 'cleaned_chunk_265', 'cleaned_chunk_266', 'cleaned_chunk_267', 'cleaned_chunk_268', 'cleaned_chunk_269', 'cleaned_chunk_27', 'cleaned_chunk_270', 'cleaned_chunk_271', 'cleaned_chunk_272', 'cleaned_chunk_273', 'cleaned_chunk_274', 'cleaned_chunk_275', 'cleaned_chunk_276', 'cleaned_chunk_277', 'cleaned_chunk_278', 'cleaned_chunk_279', 'cleaned_chunk_28', 'cleaned_chunk_280', 'cleaned_chunk_281', 'cleaned_chunk_282', 'cleaned_chunk_283', 'cleaned_chunk_284', 'cleaned_chunk_285', 'cleaned_chunk_286', 'cleaned_chunk_287', 'cleaned_chunk_288', 'cleaned_chunk_289', 'cleaned_chunk_29', 'cleaned_chunk_290', 'cleaned_chunk_291', 'cleaned_chunk_292', 'cleaned_chunk_293', 'cleaned_chunk_294', 'cleaned_chunk_295', 'cleaned_chunk_296', 'cleaned_chunk_297', 'cleaned_chunk_298', 'cleaned_chunk_299', 'cleaned_chunk_3', 'cleaned_chunk_30', 'cleaned_chunk_300', 'cleaned_chunk_301', 'cleaned_chunk_302', 'cleaned_chunk_303', 'cleaned_chunk_304', 'cleaned_chunk_305', 'cleaned_chunk_306', 'cleaned_chunk_307', 'cleaned_chunk_308', 'cleaned_chunk_309', 'cleaned_chunk_31', 'cleaned_chunk_310', 'cleaned_chunk_311', 'cleaned_chunk_312', 'cleaned_chunk_313', 'cleaned_chunk_314', 'cleaned_chunk_315', 'cleaned_chunk_316', 'cleaned_chunk_317', 'cleaned_chunk_318', 'cleaned_chunk_319', 'cleaned_chunk_32', 'cleaned_chunk_320', 'cleaned_chunk_321', 'cleaned_chunk_322', 'cleaned_chunk_323', 'cleaned_chunk_324', 'cleaned_chunk_325', 'cleaned_chunk_326', 'cleaned_chunk_327', 'cleaned_chunk_328', 'cleaned_chunk_329', 'cleaned_chunk_33', 'cleaned_chunk_330', 'cleaned_chunk_331', 'cleaned_chunk_332', 'cleaned_chunk_333', 'cleaned_chunk_334', 'cleaned_chunk_335', 'cleaned_chunk_336', 'cleaned_chunk_337', 'cleaned_chunk_338', 'cleaned_chunk_339', 'cleaned_chunk_34', 'cleaned_chunk_340', 'cleaned_chunk_341', 'cleaned_chunk_342', 'cleaned_chunk_343', 'cleaned_chunk_344', 'cleaned_chunk_345', 'cleaned_chunk_346', 'cleaned_chunk_347', 'cleaned_chunk_348', 'cleaned_chunk_349', 'cleaned_chunk_35', 'cleaned_chunk_350', 'cleaned_chunk_351', 'cleaned_chunk_352', 'cleaned_chunk_353', 'cleaned_chunk_354', 'cleaned_chunk_355', 'cleaned_chunk_356', 'cleaned_chunk_357', 'cleaned_chunk_358', 'cleaned_chunk_359', 'cleaned_chunk_36', 'cleaned_chunk_360', 'cleaned_chunk_361', 'cleaned_chunk_362', 'cleaned_chunk_363', 'cleaned_chunk_364', 'cleaned_chunk_365', 'cleaned_chunk_366', 'cleaned_chunk_367', 'cleaned_chunk_368', 'cleaned_chunk_369', 'cleaned_chunk_37', 'cleaned_chunk_370', 'cleaned_chunk_371', 'cleaned_chunk_372', 'cleaned_chunk_373', 'cleaned_chunk_374', 'cleaned_chunk_375', 'cleaned_chunk_376', 'cleaned_chunk_377', 'cleaned_chunk_378', 'cleaned_chunk_379', 'cleaned_chunk_38', 'cleaned_chunk_380', 'cleaned_chunk_381', 'cleaned_chunk_382', 'cleaned_chunk_383', 'cleaned_chunk_384', 'cleaned_chunk_385', 'cleaned_chunk_386', 'cleaned_chunk_387', 'cleaned_chunk_388', 'cleaned_chunk_389', 'cleaned_chunk_39', 'cleaned_chunk_390', 'cleaned_chunk_391', 'cleaned_chunk_392', 'cleaned_chunk_393', 'cleaned_chunk_394', 'cleaned_chunk_395', 'cleaned_chunk_396', 'cleaned_chunk_397', 'cleaned_chunk_398', 'cleaned_chunk_399', 'cleaned_chunk_4', 'cleaned_chunk_40', 'cleaned_chunk_400', 'cleaned_chunk_401', 'cleaned_chunk_402', 'cleaned_chunk_403', 'cleaned_chunk_404', 'cleaned_chunk_405', 'cleaned_chunk_406', 'cleaned_chunk_407', 'cleaned_chunk_408', 'cleaned_chunk_409', 'cleaned_chunk_41', 'cleaned_chunk_410', 'cleaned_chunk_411', 'cleaned_chunk_412', 'cleaned_chunk_413', 'cleaned_chunk_414', 'cleaned_chunk_415', 'cleaned_chunk_416', 'cleaned_chunk_417', 'cleaned_chunk_418', 'cleaned_chunk_419', 'cleaned_chunk_42', 'cleaned_chunk_420', 'cleaned_chunk_421', 'cleaned_chunk_422', 'cleaned_chunk_423', 'cleaned_chunk_424', 'cleaned_chunk_425', 'cleaned_chunk_426', 'cleaned_chunk_427', 'cleaned_chunk_428', 'cleaned_chunk_429', 'cleaned_chunk_43', 'cleaned_chunk_430', 'cleaned_chunk_431', 'cleaned_chunk_432', 'cleaned_chunk_433', 'cleaned_chunk_434', 'cleaned_chunk_435', 'cleaned_chunk_436', 'cleaned_chunk_437', 'cleaned_chunk_438', 'cleaned_chunk_439', 'cleaned_chunk_44', 'cleaned_chunk_440', 'cleaned_chunk_441', 'cleaned_chunk_442', 'cleaned_chunk_443', 'cleaned_chunk_444', 'cleaned_chunk_445', 'cleaned_chunk_446', 'cleaned_chunk_447', 'cleaned_chunk_448', 'cleaned_chunk_449', 'cleaned_chunk_45', 'cleaned_chunk_450', 'cleaned_chunk_451', 'cleaned_chunk_452', 'cleaned_chunk_453', 'cleaned_chunk_454', 'cleaned_chunk_455', 'cleaned_chunk_456', 'cleaned_chunk_457', 'cleaned_chunk_458', 'cleaned_chunk_459', 'cleaned_chunk_46', 'cleaned_chunk_460', 'cleaned_chunk_461', 'cleaned_chunk_462', 'cleaned_chunk_463', 'cleaned_chunk_464', 'cleaned_chunk_465', 'cleaned_chunk_466', 'cleaned_chunk_467', 'cleaned_chunk_468', 'cleaned_chunk_469', 'cleaned_chunk_47', 'cleaned_chunk_470', 'cleaned_chunk_471', 'cleaned_chunk_472', 'cleaned_chunk_473', 'cleaned_chunk_474', 'cleaned_chunk_475', 'cleaned_chunk_476', 'cleaned_chunk_477', 'cleaned_chunk_478', 'cleaned_chunk_479', 'cleaned_chunk_48', 'cleaned_chunk_480', 'cleaned_chunk_481', 'cleaned_chunk_482', 'cleaned_chunk_483', 'cleaned_chunk_484', 'cleaned_chunk_485', 'cleaned_chunk_486', 'cleaned_chunk_487', 'cleaned_chunk_488', 'cleaned_chunk_489', 'cleaned_chunk_49', 'cleaned_chunk_490', 'cleaned_chunk_491', 'cleaned_chunk_492', 'cleaned_chunk_493', 'cleaned_chunk_494', 'cleaned_chunk_495', 'cleaned_chunk_496', 'cleaned_chunk_497', 'cleaned_chunk_498', 'cleaned_chunk_499', 'cleaned_chunk_5', 'cleaned_chunk_50', 'cleaned_chunk_500', 'cleaned_chunk_501', 'cleaned_chunk_502', 'cleaned_chunk_503', 'cleaned_chunk_504', 'cleaned_chunk_505', 'cleaned_chunk_506', 'cleaned_chunk_507', 'cleaned_chunk_508', 'cleaned_chunk_509', 'cleaned_chunk_51', 'cleaned_chunk_510', 'cleaned_chunk_511', 'cleaned_chunk_512', 'cleaned_chunk_513', 'cleaned_chunk_514', 'cleaned_chunk_515', 'cleaned_chunk_516', 'cleaned_chunk_517', 'cleaned_chunk_518', 'cleaned_chunk_519', 'cleaned_chunk_52', 'cleaned_chunk_520', 'cleaned_chunk_521', 'cleaned_chunk_522', 'cleaned_chunk_523', 'cleaned_chunk_524', 'cleaned_chunk_525', 'cleaned_chunk_526', 'cleaned_chunk_527', 'cleaned_chunk_528', 'cleaned_chunk_529', 'cleaned_chunk_53', 'cleaned_chunk_530', 'cleaned_chunk_531', 'cleaned_chunk_532', 'cleaned_chunk_533', 'cleaned_chunk_534', 'cleaned_chunk_535', 'cleaned_chunk_536', 'cleaned_chunk_537', 'cleaned_chunk_538', 'cleaned_chunk_539', 'cleaned_chunk_54', 'cleaned_chunk_540', 'cleaned_chunk_541', 'cleaned_chunk_542', 'cleaned_chunk_543', 'cleaned_chunk_544', 'cleaned_chunk_545', 'cleaned_chunk_546', 'cleaned_chunk_547', 'cleaned_chunk_548', 'cleaned_chunk_549', 'cleaned_chunk_55', 'cleaned_chunk_550', 'cleaned_chunk_551', 'cleaned_chunk_552', 'cleaned_chunk_553', 'cleaned_chunk_554', 'cleaned_chunk_555', 'cleaned_chunk_556', 'cleaned_chunk_557', 'cleaned_chunk_558', 'cleaned_chunk_559', 'cleaned_chunk_56', 'cleaned_chunk_560', 'cleaned_chunk_561', 'cleaned_chunk_562', 'cleaned_chunk_563', 'cleaned_chunk_564', 'cleaned_chunk_565', 'cleaned_chunk_566', 'cleaned_chunk_567', 'cleaned_chunk_568', 'cleaned_chunk_569', 'cleaned_chunk_57', 'cleaned_chunk_570', 'cleaned_chunk_571', 'cleaned_chunk_572', 'cleaned_chunk_573', 'cleaned_chunk_574', 'cleaned_chunk_575', 'cleaned_chunk_576', 'cleaned_chunk_577', 'cleaned_chunk_578', 'cleaned_chunk_579', 'cleaned_chunk_58', 'cleaned_chunk_580', 'cleaned_chunk_581', 'cleaned_chunk_582', 'cleaned_chunk_583', 'cleaned_chunk_584', 'cleaned_chunk_585', 'cleaned_chunk_586', 'cleaned_chunk_587', 'cleaned_chunk_588', 'cleaned_chunk_589', 'cleaned_chunk_59', 'cleaned_chunk_590', 'cleaned_chunk_591', 'cleaned_chunk_592', 'cleaned_chunk_593', 'cleaned_chunk_594', 'cleaned_chunk_595', 'cleaned_chunk_596', 'cleaned_chunk_597', 'cleaned_chunk_598', 'cleaned_chunk_599', 'cleaned_chunk_6', 'cleaned_chunk_60', 'cleaned_chunk_600', 'cleaned_chunk_601', 'cleaned_chunk_602', 'cleaned_chunk_603', 'cleaned_chunk_604', 'cleaned_chunk_605', 'cleaned_chunk_606', 'cleaned_chunk_607', 'cleaned_chunk_608', 'cleaned_chunk_609', 'cleaned_chunk_61', 'cleaned_chunk_610', 'cleaned_chunk_611', 'cleaned_chunk_612', 'cleaned_chunk_613', 'cleaned_chunk_614', 'cleaned_chunk_615', 'cleaned_chunk_616', 'cleaned_chunk_617', 'cleaned_chunk_618', 'cleaned_chunk_619', 'cleaned_chunk_62', 'cleaned_chunk_620', 'cleaned_chunk_621', 'cleaned_chunk_622', 'cleaned_chunk_623', 'cleaned_chunk_624', 'cleaned_chunk_625', 'cleaned_chunk_626', 'cleaned_chunk_627', 'cleaned_chunk_628', 'cleaned_chunk_629', 'cleaned_chunk_63', 'cleaned_chunk_630', 'cleaned_chunk_631', 'cleaned_chunk_632', 'cleaned_chunk_633', 'cleaned_chunk_634', 'cleaned_chunk_635', 'cleaned_chunk_636', 'cleaned_chunk_637', 'cleaned_chunk_638', 'cleaned_chunk_639', 'cleaned_chunk_64', 'cleaned_chunk_640', 'cleaned_chunk_641', 'cleaned_chunk_642', 'cleaned_chunk_643', 'cleaned_chunk_644', 'cleaned_chunk_645', 'cleaned_chunk_646', 'cleaned_chunk_647', 'cleaned_chunk_648', 'cleaned_chunk_649', 'cleaned_chunk_65', 'cleaned_chunk_650', 'cleaned_chunk_651', 'cleaned_chunk_652', 'cleaned_chunk_653', 'cleaned_chunk_654', 'cleaned_chunk_655', 'cleaned_chunk_656', 'cleaned_chunk_657', 'cleaned_chunk_658', 'cleaned_chunk_659', 'cleaned_chunk_66', 'cleaned_chunk_660', 'cleaned_chunk_661', 'cleaned_chunk_662', 'cleaned_chunk_663', 'cleaned_chunk_664', 'cleaned_chunk_665', 'cleaned_chunk_666', 'cleaned_chunk_667', 'cleaned_chunk_668', 'cleaned_chunk_669', 'cleaned_chunk_67', 'cleaned_chunk_670', 'cleaned_chunk_671', 'cleaned_chunk_672', 'cleaned_chunk_673', 'cleaned_chunk_674', 'cleaned_chunk_675', 'cleaned_chunk_676', 'cleaned_chunk_677', 'cleaned_chunk_678', 'cleaned_chunk_679', 'cleaned_chunk_68', 'cleaned_chunk_680', 'cleaned_chunk_681', 'cleaned_chunk_682', 'cleaned_chunk_683', 'cleaned_chunk_684', 'cleaned_chunk_685', 'cleaned_chunk_686', 'cleaned_chunk_687', 'cleaned_chunk_688', 'cleaned_chunk_689', 'cleaned_chunk_69', 'cleaned_chunk_690', 'cleaned_chunk_691', 'cleaned_chunk_692', 'cleaned_chunk_693', 'cleaned_chunk_694', 'cleaned_chunk_695', 'cleaned_chunk_696', 'cleaned_chunk_697', 'cleaned_chunk_698', 'cleaned_chunk_699', 'cleaned_chunk_7', 'cleaned_chunk_70', 'cleaned_chunk_700', 'cleaned_chunk_701', 'cleaned_chunk_702', 'cleaned_chunk_703', 'cleaned_chunk_704', 'cleaned_chunk_705', 'cleaned_chunk_706', 'cleaned_chunk_707', 'cleaned_chunk_708', 'cleaned_chunk_709', 'cleaned_chunk_71', 'cleaned_chunk_710', 'cleaned_chunk_711', 'cleaned_chunk_712', 'cleaned_chunk_713', 'cleaned_chunk_714', 'cleaned_chunk_715', 'cleaned_chunk_716', 'cleaned_chunk_717', 'cleaned_chunk_718', 'cleaned_chunk_719', 'cleaned_chunk_72', 'cleaned_chunk_720', 'cleaned_chunk_721', 'cleaned_chunk_722', 'cleaned_chunk_723', 'cleaned_chunk_724', 'cleaned_chunk_725', 'cleaned_chunk_726', 'cleaned_chunk_727', 'cleaned_chunk_728', 'cleaned_chunk_729', 'cleaned_chunk_73', 'cleaned_chunk_730', 'cleaned_chunk_731', 'cleaned_chunk_732', 'cleaned_chunk_733', 'cleaned_chunk_734', 'cleaned_chunk_735', 'cleaned_chunk_736', 'cleaned_chunk_737', 'cleaned_chunk_738', 'cleaned_chunk_739', 'cleaned_chunk_74', 'cleaned_chunk_740', 'cleaned_chunk_741', 'cleaned_chunk_742', 'cleaned_chunk_743', 'cleaned_chunk_744', 'cleaned_chunk_745', 'cleaned_chunk_746', 'cleaned_chunk_747', 'cleaned_chunk_748', 'cleaned_chunk_749', 'cleaned_chunk_75', 'cleaned_chunk_750', 'cleaned_chunk_751', 'cleaned_chunk_752', 'cleaned_chunk_753', 'cleaned_chunk_754', 'cleaned_chunk_76', 'cleaned_chunk_77', 'cleaned_chunk_78', 'cleaned_chunk_79', 'cleaned_chunk_8', 'cleaned_chunk_80', 'cleaned_chunk_81', 'cleaned_chunk_82', 'cleaned_chunk_83', 'cleaned_chunk_84', 'cleaned_chunk_85', 'cleaned_chunk_86', 'cleaned_chunk_87', 'cleaned_chunk_88', 'cleaned_chunk_89', 'cleaned_chunk_9', 'cleaned_chunk_90', 'cleaned_chunk_91', 'cleaned_chunk_92', 'cleaned_chunk_93', 'cleaned_chunk_94', 'cleaned_chunk_95', 'cleaned_chunk_96', 'cleaned_chunk_97', 'cleaned_chunk_98', 'cleaned_chunk_99']\n",
      "Total chunks to process: 755\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"./datasets/snippets/\"\n",
    "progress_file = \"./progress.json\" \n",
    "\n",
    "# Load progress from file, if it exists\n",
    "def load_progress():\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\"chunk\": None, \"last_row_idx\": 0}\n",
    "\n",
    "# Save progress to file\n",
    "def save_progress(chunk_dir, row_idx):\n",
    "    progress = {\"chunk\": chunk_dir, \"last_row_idx\": row_idx}\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump(progress, f)\n",
    "    print(f\"Progress saved: {chunk_dir}, row {row_idx}\", end=\"\\r\")\n",
    "\n",
    "    \n",
    "# List all the subdirectories in the dataset directory\n",
    "chunk_dirs = [f for f in os.listdir(dataset_dir) \n",
    "              if os.path.isdir(os.path.join(dataset_dir, f)) and f.startswith('cleaned_chunk_')]\n",
    "\n",
    "print(\"Found directories:\", chunk_dirs)\n",
    "total_chunks = len(chunk_dirs)\n",
    "print(f\"Total chunks to process: {total_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to process: 755\n",
      "\tCurrent chunks: cleaned_chunk_0\n",
      "\tCurrent Index: 6563\n"
     ]
    }
   ],
   "source": [
    "# # Load the last progress (if any)\n",
    "# progress = load_progress()\n",
    "\n",
    "# if progress:\n",
    "#     start_chunk = progress['chunk']\n",
    "#     start_row_idx = progress['last_row_idx']\n",
    "#     # Load the last progress (if any)\n",
    "# else:\n",
    "#     print(\"Starting from the beginning...\")\n",
    "#     start_chunk = None\n",
    "#     start_row_idx = None\n",
    "\n",
    "\n",
    "progress = load_progress()\n",
    "start_chunk = progress.get('chunk')\n",
    "start_row_idx = progress.get('last_row_idx', 0)\n",
    "# Total number of chunks\n",
    "total_chunks = len(chunk_dirs)\n",
    "print(f\"Total chunks to process: {total_chunks}\")\n",
    "print(f\"\\tCurrent chunks: {start_chunk}\")\n",
    "print(f\"\\tCurrent Index: {start_row_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cleaned_chunk_1:  16%|████▍                        | 1551/10000 [07:07<56:25,  2.50row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved: cleaned_chunk_1, row 1551\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cleaned_chunk_1:  16%|████▎                      | 1601/10000 [07:26<1:01:14,  2.29row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved: cleaned_chunk_1, row 1601\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cleaned_chunk_1:  17%|████▊                        | 1651/10000 [07:47<51:52,  2.68row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved: cleaned_chunk_1, row 1651\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cleaned_chunk_1:  17%|████▉                        | 1701/10000 [08:06<56:40,  2.44row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved: cleaned_chunk_1, row 1701\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cleaned_chunk_1:  18%|█████                        | 1751/10000 [08:27<54:35,  2.52row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved: cleaned_chunk_1, row 1751\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cleaned_chunk_1:  18%|████▊                      | 1801/10000 [08:57<2:17:11,  1.00s/row]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved: cleaned_chunk_1, row 1801\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cleaned_chunk_1:  18%|████▉                      | 1807/10000 [09:00<1:20:51,  1.69row/s]Exception occurred invoking consumer for subscription b90ef180c8d3423d97d10fe0f92bb295to topic persistent://default/default/d2a3e480-de6c-4bee-84e3-95ca4a05e2d2 \n",
      "Processing cleaned_chunk_1:  18%|████▉                      | 1833/10000 [09:16<1:17:54,  1.75row/s]"
     ]
    }
   ],
   "source": [
    "# Process each chunk\n",
    "for idx, chunk_dir in enumerate(chunk_dirs, 1):\n",
    "    dataset_chunk = Dataset.load_from_disk(os.path.join(dataset_dir, chunk_dir))\n",
    "    total_rows = len(dataset_chunk)\n",
    "\n",
    "    # Resume from the correct row if restarting\n",
    "    if start_chunk and chunk_dir == start_chunk:\n",
    "        if start_row_idx >= total_rows:\n",
    "            print(f\"Skipping {chunk_dir}, start_row_idx exceeds total_rows.\")\n",
    "            save_progress(chunk_dir, 0)\n",
    "            start_row_idx = 0\n",
    "            start_chunk = None\n",
    "            continue\n",
    "\n",
    "        # Resume processing the current chunk\n",
    "        print(f\"Resuming {chunk_dir} from row {start_row_idx}\")\n",
    "        rows_to_process = dataset_chunk[start_row_idx:]\n",
    "    else:\n",
    "        start_row_idx = 0  # Reset start_row_idx for new chunks\n",
    "        rows_to_process = dataset_chunk\n",
    "\n",
    "    # Process rows in the current chunk\n",
    "    for row_idx, first_row in enumerate(tqdm(rows_to_process, desc=f\"Processing {chunk_dir}\", \n",
    "                                             total=total_rows - start_row_idx, ncols=100, unit=\"row\"), \n",
    "                                           start=start_row_idx):\n",
    "        try:\n",
    "            ids = [str(first_row['id'])]\n",
    "            documents = [first_row['snippet']]\n",
    "            metadatas = [{\n",
    "                \"type\": \"snippet\",\n",
    "                \"repo\": first_row['repo_file_name'],\n",
    "                \"language\": first_row['language']\n",
    "            }]\n",
    "\n",
    "            # Avoid duplicates\n",
    "            existing_ids = collection.get(ids=ids)['ids']\n",
    "            if not existing_ids:\n",
    "                collection.add(\n",
    "                    ids=ids,\n",
    "                    documents=documents,\n",
    "                    metadatas=metadatas\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {row_idx} in {chunk_dir}: {e}\")\n",
    "\n",
    "        # Save progress every 50 rows\n",
    "        if row_idx % 50 == 0:\n",
    "            save_progress(chunk_dir, row_idx + 1)\n",
    "\n",
    "    # After finishing a chunk, reset progress\n",
    "    save_progress(chunk_dir, 0)\n",
    "    start_row_idx = 0\n",
    "    start_chunk = None\n",
    "\n",
    "    # Display progress\n",
    "    chunk_progress = (idx / total_chunks) * 100\n",
    "    print(f\"\\033[KProcessed {idx}/{total_chunks} chunks ({chunk_progress:.2f}%)\", end=\"\\r\")\n",
    "\n",
    "print(\"\\n✅ All chunks have been added to the collection successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['88608',\n",
       "  '88609',\n",
       "  '88610',\n",
       "  '88611',\n",
       "  '88612',\n",
       "  '88613',\n",
       "  '88614',\n",
       "  '88615',\n",
       "  '88616',\n",
       "  '88617'],\n",
       " 'embeddings': array([[-7.49901228e-05,  1.13361422e-02, -9.46090091e-03, ...,\n",
       "         -1.62886903e-02,  2.23879162e-02, -5.50048575e-02],\n",
       "        [ 3.67557779e-02, -2.23476384e-02, -4.64685336e-02, ...,\n",
       "          6.27410263e-02,  4.71353680e-02, -4.86179851e-02],\n",
       "        [-6.68321401e-02,  9.62347910e-03, -3.16255689e-02, ...,\n",
       "          1.55157335e-02,  1.53038085e-01, -5.40957153e-02],\n",
       "        ...,\n",
       "        [ 6.40597418e-02,  2.80569047e-02, -1.88555662e-02, ...,\n",
       "          7.03850985e-02, -6.30347878e-02, -2.12625600e-02],\n",
       "        [-4.71711643e-02, -4.86637540e-02, -1.74724516e-02, ...,\n",
       "          1.06600709e-02,  5.52940881e-04, -3.09259407e-02],\n",
       "        [-5.24540320e-02,  2.24042162e-02, -2.29494162e-02, ...,\n",
       "          7.10451677e-02, -1.19481666e-03,  3.72953597e-03]],\n",
       "       shape=(10, 384)),\n",
       " 'documents': ['/*\\r\\n * Copyright 2014 The Netty Project\\r\\n *\\r\\n * The Netty Project licenses this file to you under the Apache License,\\r\\n * version 2.0 (the \"License\"); you may not use this file except in compliance\\r\\n',\n",
       "  ' * with the License. You may obtain a copy of the License at:\\r\\n *\\r\\n *   https://www.apache.org/licenses/LICENSE-2.0\\r\\n *\\r\\n * Unless required by applicable law or agreed to in writing, software\\r\\n',\n",
       "  ' * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\\r\\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\r\\n * License for the specific language governing permissions and limitations\\r\\n * under the License.\\r\\n */\\r\\n',\n",
       "  '\\r\\npackage io.netty.resolver.dns;\\r\\n\\r\\nimport io.netty.util.NetUtil;\\r\\nimport org.junit.Test;\\r\\n',\n",
       "  '\\r\\nimport java.net.InetSocketAddress;\\r\\nimport java.util.Collections;\\r\\nimport java.util.IdentityHashMap;\\r\\nimport java.util.Set;\\r\\n',\n",
       "  '\\r\\nimport static io.netty.resolver.dns.DefaultDnsServerAddressStreamProvider.defaultAddressList;\\r\\nimport static org.hamcrest.MatcherAssert.assertThat;\\r\\nimport static org.hamcrest.Matchers.*;\\r\\n\\r\\n',\n",
       "  'public class DnsServerAddressesTest {\\r\\n\\r\\n    private static final InetSocketAddress ADDR1 = new InetSocketAddress(NetUtil.LOCALHOST, 1);\\r\\n    private static final InetSocketAddress ADDR2 = new InetSocketAddress(NetUtil.LOCALHOST, 2);\\r\\n    private static final InetSocketAddress ADDR3 = new InetSocketAddress(NetUtil.LOCALHOST, 3);\\r\\n',\n",
       "  '\\r\\n    @Test\\r\\n    public void testDefaultAddresses() {\\r\\n        assertThat(defaultAddressList().size(), is(greaterThan(0)));\\r\\n    }\\r\\n',\n",
       "  '\\r\\n    @Test\\r\\n    public void testSequential() {\\r\\n        DnsServerAddresses seq = DnsServerAddresses.sequential(ADDR1, ADDR2, ADDR3);\\r\\n        assertThat(seq.stream(), is(not(sameInstance(seq.stream()))));\\r\\n',\n",
       "  '\\r\\n        for (int j = 0; j < 2; j ++) {\\r\\n            DnsServerAddressStream i = seq.stream();\\r\\n            assertNext(i, ADDR1);\\r\\n            assertNext(i, ADDR2);\\r\\n'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'},\n",
       "  {'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'},\n",
       "  {'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'},\n",
       "  {'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'},\n",
       "  {'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'},\n",
       "  {'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'},\n",
       "  {'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'},\n",
       "  {'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'},\n",
       "  {'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'},\n",
       "  {'language': 'Java',\n",
       "   'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DnsServerAddressesTest.java',\n",
       "   'type': 'snippet'}],\n",
       " 'included': [<IncludeEnum.embeddings: 'embeddings'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the collection: 10375\n"
     ]
    }
   ],
   "source": [
    "# Get the number of items in the collection\n",
    "num_items = collection.count()\n",
    "\n",
    "print(f\"Number of items in the collection: {num_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 5: After the collection is ready, we can query it using the query() method of the collection and print the results returned by the query().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['88884']], 'embeddings': None, 'documents': [['                    @Override\\r\\n                    public int compare(InetSocketAddress o1, InetSocketAddress o2) {\\r\\n                        if (o1.equals(o2)) {\\r\\n                            return 0;\\r\\n                        }\\r\\n']], 'uris': None, 'data': None, 'metadatas': [[{'language': 'Java', 'repo': 'netty/netty/resolver-dns/src/test/java/io/netty/resolver/dns/DefaultAuthoritativeDnsServerCacheTest.java', 'type': 'snippet'}]], 'distances': [[1.0026335716247559]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(query_texts=\"compareTo function\", n_results=1)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10000 is greater than number of elements in index 3696, updating n_results = 3696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire collection data saved to ./JavaCodeSnippetsDB\\full_collection_data.json\n"
     ]
    }
   ],
   "source": [
    "# # Use a query without any filters to fetch all items\n",
    "# results = collection.query(query_texts=\"*\", n_results=10000)  # Adjust the n_results as needed (limit)\n",
    "\n",
    "# # Ensure the directory exists\n",
    "# directory_path = './JavaCodeSnippetsDB'\n",
    "# if not os.path.exists(directory_path):\n",
    "#     os.makedirs(directory_path)\n",
    "\n",
    "# # Define the path for the file to save the results\n",
    "# file_path = os.path.join(directory_path, 'full_collection_data.json')\n",
    "\n",
    "# # Save the entire collection data to a JSON file\n",
    "# with open(file_path, 'w') as f:\n",
    "#     json.dump(results, f)\n",
    "\n",
    "# print(f\"Entire collection data saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
