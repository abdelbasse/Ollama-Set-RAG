{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle API Setup Guide\n",
    "\n",
    "This guide explains how to set up the Kaggle API to use Python for downloading datasets.\n",
    "\n",
    "## Prerequisites\n",
    "- Ensure Python is installed on your system.\n",
    "- Install the Kaggle API package by running:\n",
    "  ```\n",
    "  pip install kaggle\n",
    "  ```\n",
    "\n",
    "## Setting Up `kaggle.json`\n",
    "The `kaggle.json` file contains your API key, which is required to authenticate with Kaggle.\n",
    "\n",
    "### Step 1: Download `kaggle.json`\n",
    "1. Log in to your [Kaggle account settings](https://www.kaggle.com/account).\n",
    "2. Scroll to the **API** section.\n",
    "3. Click **Create New API Token**. This will download a file named `kaggle.json`.\n",
    "\n",
    "### Step 2: Place the `kaggle.json` File\n",
    "1. Create a `.kaggle` directory in your user home directory:\n",
    "   - On Windows: `C:\\Users\\YourUsername\\.kaggle`\n",
    "   - On Mac/Linux: `~/.kaggle`\n",
    "\n",
    "2. Move the `kaggle.json` file to the `.kaggle` directory.\n",
    "\n",
    "### Step 3: Verify Permissions (Optional for Windows)\n",
    "- On Linux/Mac, set the correct permissions to secure the file:\n",
    "  ```\n",
    "  chmod 600 ~/.kaggle/kaggle.json\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 'simiotic/github-code-snippets'...\n",
      "Dataset URL: https://www.kaggle.com/datasets/simiotic/github-code-snippets\n",
      "Dataset downloaded and extracted to: /root/Ollama-Set-RAG/datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Initialize and authenticate the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Specify the dataset to download\n",
    "dataset_name = \"simiotic/github-code-snippets\"  # Replace with your desired dataset\n",
    "\n",
    "# Define the path to download the dataset\n",
    "download_path = \"./datasets\"\n",
    "\n",
    "# Download the dataset\n",
    "print(f\"Downloading dataset '{dataset_name}'...\")\n",
    "api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
    "print(f\"Dataset downloaded and extracted to: {os.path.abspath(download_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the data of this `.db` file\n",
    "### Step 1: Download `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./RagDataSetup/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./RagDataSetup/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.0 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Show the data\n",
    "Since your dataset is located in the ./datasets/snippets/ directory and you have a file named snippets.db, it seems like the dataset might be in an SQLite database format. You can use Python to load the data from the snippets.db file using the sqlite3 library in Python.\n",
    "\n",
    "Here's how you can access the data from the snippets.db SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      language  total_chunk_size\n",
      "0         Bash             53840\n",
      "1            C          61905695\n",
      "2          C++          32807530\n",
      "3          CSV            888775\n",
      "4      DOTFILE            337085\n",
      "5           Go          40669595\n",
      "6         HTML           9936365\n",
      "7         JSON          32576945\n",
      "8         Java          37707120\n",
      "9   JavaScript          45340285\n",
      "10     Jupyter           1229605\n",
      "11    Markdown          14949355\n",
      "12  PowerShell            195980\n",
      "13      Python          19833355\n",
      "14        Ruby           5435615\n",
      "15        Rust           2993515\n",
      "16       Shell           1923690\n",
      "17         TSV             51525\n",
      "18        Text          17147775\n",
      "19     UNKNOWN         158415145\n",
      "20        YAML           2938680\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the SQLite database\n",
    "database_path = \"./datasets/snippets/snippets.db\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(database_path)\n",
    "\n",
    "# Query to select all columns from the 'snippets' table\n",
    "query = \"SELECT language, SUM(chunk_size) AS total_chunk_size FROM snippets GROUP BY language\"  # You can adjust the LIMIT as needed\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To filter the Java-related snippets from your large database and save them in a more accessible format, here's the plan:\n",
    "\n",
    "1. **Filter Java-related Snippets**: You'll extract only the rows where the `language` is \"Java\".\n",
    "2. **Export to an Efficient Format**: Save the filtered data in a format that is easy to read and process, such as **CSV** or **Parquet**, as both formats are fast to load and can be used for future analysis or requests.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Filter the Data**: Query the database for all Java-related snippets.\n",
    "2. **Export the Filtered Data**: Save the filtered snippets to a file (either CSV or Parquet).\n",
    "3. **Load and Work with the Data**: You can later load this file into pandas or any other tool for further processing.\n",
    "\n",
    "### 1. Python Code to Filter Java Snippets and Export to CSV or Parquet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java-related snippets exported to 'javaSnippets.csv'\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the SQLite database\n",
    "database_path = \"./datasets/snippets/snippets.db\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(database_path)\n",
    "\n",
    "# Query to filter Java-related snippets\n",
    "query = \"\"\"\n",
    "    SELECT * FROM snippets\n",
    "    WHERE language = 'Java'\n",
    "\"\"\"\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df_java = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Export the filtered data to CSV (or use Parquet for more efficient storage)\n",
    "df_java.to_csv(\"./datasets/snippets/javaSnippets.csv\", index=False)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Java-related snippets exported to 'javaSnippets.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explanation of the Code:\n",
    "\n",
    "- **Database Connection**: We open a connection to the `snippets.db` database using `sqlite3`.\n",
    "- **SQL Query**: We query the `snippets` table to select only those rows where the `language` is \"Java\".\n",
    "- **Exporting**: We save the filtered data to a **CSV** file (`javaSnippets.csv`) or **Parquet** file. You can choose which format works best for you. CSV is human-readable, while Parquet is more efficient for large datasets.\n",
    "- **Close the Connection**: The database connection is closed once the export is complete.\n",
    "\n",
    "### 3. Loading the Exported Data:\n",
    "\n",
    "Once you’ve exported the data to a file, you can easily load it back into pandas for any further analysis.\n",
    "\n",
    "#### Loading the CSV File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in e:\\desktop\\schoole\\enset\\bdcc-2\\semester 3\\poo java\\@@project java llm\\test\\file pdf md test\\ollama-set-rag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.5 MB 1.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.0/11.5 MB 1.4 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.0/11.5 MB 1.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.8/11.5 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.1/11.5 MB 1.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.4/11.5 MB 1.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.4/11.5 MB 1.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.6/11.5 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 1.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 1.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.2/11.5 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.5/11.5 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 5.2/11.5 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.5/11.5 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 5.8/11.5 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.0/11.5 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.0/11.5 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.3/11.5 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.3/11.5 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 1.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.1/11.5 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.3/11.5 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.3/11.5 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.6/11.5 MB 1.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.9/11.5 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.7/11.5 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.7/11.5 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.7/11.5 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.7/11.5 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.9/11.5 MB 973.0 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.2/11.5 MB 968.5 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.2/11.5 MB 968.5 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.4/11.5 MB 964.2 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 9.7/11.5 MB 955.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.7/11.5 MB 955.7 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 958.0 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 951.5 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 951.5 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.5/11.5 MB 948.3 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 10.7/11.5 MB 949.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.5 MB 949.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.5 MB 944.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 937.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 937.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 934.4 kB/s eta 0:00:00\n",
      "Using cached numpy-2.2.0-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.0 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>snippet</th>\n",
       "      <th>language</th>\n",
       "      <th>repo_file_name</th>\n",
       "      <th>github_repo_url</th>\n",
       "      <th>license</th>\n",
       "      <th>commit_hash</th>\n",
       "      <th>starting_line_number</th>\n",
       "      <th>chunk_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88608</td>\n",
       "      <td>/*\\r\\n * Copyright 2014 The Netty Project\\r\\n ...</td>\n",
       "      <td>Java</td>\n",
       "      <td>netty/netty/resolver-dns/src/test/java/io/nett...</td>\n",
       "      <td>https://github.com/netty/netty</td>\n",
       "      <td>Apache-2.0</td>\n",
       "      <td>a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88609</td>\n",
       "      <td>* with the License. You may obtain a copy of ...</td>\n",
       "      <td>Java</td>\n",
       "      <td>netty/netty/resolver-dns/src/test/java/io/nett...</td>\n",
       "      <td>https://github.com/netty/netty</td>\n",
       "      <td>Apache-2.0</td>\n",
       "      <td>a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88610</td>\n",
       "      <td>* distributed under the License is distribute...</td>\n",
       "      <td>Java</td>\n",
       "      <td>netty/netty/resolver-dns/src/test/java/io/nett...</td>\n",
       "      <td>https://github.com/netty/netty</td>\n",
       "      <td>Apache-2.0</td>\n",
       "      <td>a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88611</td>\n",
       "      <td>\\r\\npackage io.netty.resolver.dns;\\r\\n\\r\\nimpo...</td>\n",
       "      <td>Java</td>\n",
       "      <td>netty/netty/resolver-dns/src/test/java/io/nett...</td>\n",
       "      <td>https://github.com/netty/netty</td>\n",
       "      <td>Apache-2.0</td>\n",
       "      <td>a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88612</td>\n",
       "      <td>\\r\\nimport java.net.InetSocketAddress;\\r\\nimpo...</td>\n",
       "      <td>Java</td>\n",
       "      <td>netty/netty/resolver-dns/src/test/java/io/nett...</td>\n",
       "      <td>https://github.com/netty/netty</td>\n",
       "      <td>Apache-2.0</td>\n",
       "      <td>a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            snippet language  \\\n",
       "0  88608  /*\\r\\n * Copyright 2014 The Netty Project\\r\\n ...     Java   \n",
       "1  88609   * with the License. You may obtain a copy of ...     Java   \n",
       "2  88610   * distributed under the License is distribute...     Java   \n",
       "3  88611  \\r\\npackage io.netty.resolver.dns;\\r\\n\\r\\nimpo...     Java   \n",
       "4  88612  \\r\\nimport java.net.InetSocketAddress;\\r\\nimpo...     Java   \n",
       "\n",
       "                                      repo_file_name  \\\n",
       "0  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
       "1  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
       "2  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
       "3  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
       "4  netty/netty/resolver-dns/src/test/java/io/nett...   \n",
       "\n",
       "                  github_repo_url     license  \\\n",
       "0  https://github.com/netty/netty  Apache-2.0   \n",
       "1  https://github.com/netty/netty  Apache-2.0   \n",
       "2  https://github.com/netty/netty  Apache-2.0   \n",
       "3  https://github.com/netty/netty  Apache-2.0   \n",
       "4  https://github.com/netty/netty  Apache-2.0   \n",
       "\n",
       "                                    commit_hash  starting_line_number  \\\n",
       "0  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                     0   \n",
       "1  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                     5   \n",
       "2  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                    10   \n",
       "3  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                    15   \n",
       "4  a60825c3b425892af9be3e9284677aa8a58faa6b\\r\\n                    20   \n",
       "\n",
       "   chunk_size  \n",
       "0           5  \n",
       "1           5  \n",
       "2           5  \n",
       "3           5  \n",
       "4           5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Java snippets CSV file\n",
    "df_java = pd.read_csv(\"./datasets/snippets/javaSnippets.csv\")\n",
    "\n",
    "# Display the first few rows of the data\n",
    "df_java.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Benefits of This Approach:\n",
    "\n",
    "- **Speed**: Exporting the Java-related snippets to a file format like CSV or Parquet will make it faster to load and query in the future, avoiding the overhead of querying a large SQLite database.\n",
    "- **Ease of Use**: You can use pandas or other tools to read the file and easily filter, analyze, or manipulate the data.\n",
    "- **Efficient Storage**: Parquet is more space-efficient and faster for reading large datasets compared to CSV.\n",
    "\n",
    "This approach should help you manage large datasets effectively and focus on Java snippets for debugging and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm './datasets/snippets/snippets.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how you can write Python code to split a large file into two smaller files, and then combine them back into the original file:\n",
    "\n",
    "### Split the large file into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ./datasets/snippets/javaSnippets_part1.csv\n",
      "Created ./datasets/snippets/javaSnippets_part2.csv\n",
      "Created ./datasets/snippets/javaSnippets_part3.csv\n",
      "Created ./datasets/snippets/javaSnippets_part4.csv\n",
      "Created ./datasets/snippets/javaSnippets_part5.csv\n",
      "Created ./datasets/snippets/javaSnippets_part6.csv\n",
      "Created ./datasets/snippets/javaSnippets_part7.csv\n",
      "Created ./datasets/snippets/javaSnippets_part8.csv\n",
      "Created ./datasets/snippets/javaSnippets_part9.csv\n",
      "Created ./datasets/snippets/javaSnippets_part10.csv\n",
      "Created ./datasets/snippets/javaSnippets_part11.csv\n",
      "Created ./datasets/snippets/javaSnippets_part12.csv\n",
      "Created ./datasets/snippets/javaSnippets_part13.csv\n",
      "Created ./datasets/snippets/javaSnippets_part14.csv\n",
      "Created ./datasets/snippets/javaSnippets_part15.csv\n",
      "Created ./datasets/snippets/javaSnippets_part16.csv\n",
      "Created ./datasets/snippets/javaSnippets_part17.csv\n",
      "Created ./datasets/snippets/javaSnippets_part18.csv\n",
      "Created ./datasets/snippets/javaSnippets_part19.csv\n",
      "Created ./datasets/snippets/javaSnippets_part20.csv\n",
      "Created ./datasets/snippets/javaSnippets_part21.csv\n",
      "Created ./datasets/snippets/javaSnippets_part22.csv\n",
      "Created ./datasets/snippets/javaSnippets_part23.csv\n",
      "Created ./datasets/snippets/javaSnippets_part24.csv\n",
      "Created ./datasets/snippets/javaSnippets_part25.csv\n",
      "Created ./datasets/snippets/javaSnippets_part26.csv\n",
      "Created ./datasets/snippets/javaSnippets_part27.csv\n",
      "Created ./datasets/snippets/javaSnippets_part28.csv\n",
      "Created ./datasets/snippets/javaSnippets_part29.csv\n",
      "Created ./datasets/snippets/javaSnippets_part30.csv\n",
      "Created ./datasets/snippets/javaSnippets_part31.csv\n",
      "Original file ./datasets/snippets/javaSnippets.csv removed after splitting.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def split_file(input_file, part_prefix, max_size=100 * 1024 * 1024):\n",
    "    with open(input_file, 'rb') as file:\n",
    "        part_number = 1\n",
    "        while True:\n",
    "            # Read up to `max_size` bytes\n",
    "            data = file.read(max_size)\n",
    "            if not data:  # Stop if no more data to read\n",
    "                break\n",
    "            \n",
    "            # Format the output file name: <part_prefix>_part<part_number>.csv\n",
    "            part_file = f\"{part_prefix}_part{part_number}.csv\"\n",
    "            with open(part_file, 'wb') as output:\n",
    "                output.write(data)\n",
    "            \n",
    "            print(f\"Created {part_file}\")\n",
    "            part_number += 1\n",
    "\n",
    "    os.remove(input_file)  # Remove the original file\n",
    "    print(f\"Original file {input_file} removed after splitting.\")\n",
    "\n",
    "# Example usage\n",
    "path = './datasets/snippets/'\n",
    "input_file = f'{path}javaSnippets.csv'\n",
    "part_prefix = f'{path}javaSnippets'  # Prefix for output parts\n",
    "\n",
    "split_file(input_file, part_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the two smaller files back into the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ./datasets/snippets/javaSnippets_part1.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part2.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part3.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part4.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part5.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part6.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part7.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part8.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part9.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part10.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part11.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part12.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part13.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part14.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part15.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part16.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part17.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part18.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part19.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part20.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part21.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part22.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part23.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part24.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part25.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part26.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part27.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part28.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part29.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part30.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Added ./datasets/snippets/javaSnippets_part31.csv to ./datasets/snippets/javaSnippets.csv and removed it.\n",
      "Files combined into ./datasets/snippets/javaSnippets.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def combine_files(part_prefix, output_file):\n",
    "    part_number = 1\n",
    "    with open(output_file, 'wb') as output:\n",
    "        while True:\n",
    "            # Generate part file name: <part_prefix>_part<part_number>.csv\n",
    "            part_file = f\"{part_prefix}_part{part_number}.csv\"\n",
    "            \n",
    "            if not os.path.exists(part_file):\n",
    "                # Stop if the part file doesn't exist\n",
    "                break\n",
    "            \n",
    "            # Combine the part file into the output file\n",
    "            with open(part_file, 'rb') as part:\n",
    "                output.write(part.read())\n",
    "            \n",
    "            # Remove the part file after combining\n",
    "            os.remove(part_file)\n",
    "            print(f\"Added {part_file} to {output_file} and removed it.\")\n",
    "            part_number += 1\n",
    "\n",
    "    print(f\"Files combined into {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "path = './datasets/snippets/'\n",
    "part_prefix = f'{path}javaSnippets' \n",
    "output_file = f'{path}javaSnippets.csv'\n",
    "\n",
    "combine_files(part_prefix, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the code works:\n",
    "1. **Splitting**: \n",
    "   - The `split_file` function reads the entire input file as binary (`'rb'` mode).\n",
    "   - It splits the data into two parts using the midpoint (half of the file size).\n",
    "   - It writes the first half to `output_file1` and the second half to `output_file2`.\n",
    "\n",
    "2. **Combining**:\n",
    "   - The `combine_files` function opens both smaller files (`input_file1` and `input_file2`) and writes their contents into `output_file`.\n",
    "   - The files are opened in binary mode (`'rb'` and `'wb'`) to handle any file type, including large files.\n",
    "\n",
    "### Notes:\n",
    "- Replace `'path_to_large_file/javaSnippets.csv'`, `'path_to_large_file/javaSnippets_part1.csv'`, etc., with the actual paths of your files.\n",
    "- Make sure the file you want to split is not open in another program, as this might cause a permission error.\n",
    "\n",
    "Let me know if you need further assistance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp './datasets/snippets/javaSnippets.csv' './datasets/snippets/Storage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdifflib\u001b[39;00m\n\u001b[1;32m     12\u001b[0m diff \u001b[38;5;241m=\u001b[39m difflib\u001b[38;5;241m.\u001b[39munified_diff(lines1, lines2, fromfile\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjavaSnippets.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, tofile\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStorage/javaSnippets.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/difflib.py:1138\u001b[0m, in \u001b[0;36munified_diff\u001b[0;34m(a, b, fromfile, tofile, fromfiledate, tofiledate, n, lineterm)\u001b[0m\n\u001b[1;32m   1136\u001b[0m _check_types(a, b, fromfile, tofile, fromfiledate, tofiledate, lineterm)\n\u001b[1;32m   1137\u001b[0m started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSequenceMatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_grouped_opcodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstarted\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstarted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/difflib.py:572\u001b[0m, in \u001b[0;36mSequenceMatcher.get_grouped_opcodes\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_grouped_opcodes\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    548\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Isolate change clusters by eliminating ranges with no changes.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m    Return a generator of groups with up to n lines of context.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m      ('equal', 35, 38, 31, 34)]]\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_opcodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m codes:\n\u001b[1;32m    574\u001b[0m         codes \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m/usr/lib/python3.12/difflib.py:525\u001b[0m, in \u001b[0;36mSequenceMatcher.get_opcodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m i \u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopcodes \u001b[38;5;241m=\u001b[39m answer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ai, bj, size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matching_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;66;03m# invariant:  we've pumped out correct diffs to change\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;66;03m# a[:i] into b[:j], and the next matching block is\u001b[39;00m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;66;03m# a[ai:ai+size] == b[bj:bj+size].  So we need to pump\u001b[39;00m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# out a diff to change a[i:ai] into b[j:bj], pump out\u001b[39;00m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;66;03m# the matching block, and move (i,j) beyond the match\u001b[39;00m\n\u001b[1;32m    531\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m ai \u001b[38;5;129;01mand\u001b[39;00m j \u001b[38;5;241m<\u001b[39m bj:\n",
      "File \u001b[0;32m/usr/lib/python3.12/difflib.py:454\u001b[0m, in \u001b[0;36mSequenceMatcher.get_matching_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m queue:\n\u001b[1;32m    453\u001b[0m     alo, ahi, blo, bhi \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m--> 454\u001b[0m     i, j, k \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_longest_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43malo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mahi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbhi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# a[alo:i] vs b[blo:j] unknown\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# a[i:i+k] same as b[j:j+k]\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# a[i+k:ahi] vs b[j+k:bhi] unknown\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k:   \u001b[38;5;66;03m# if k is 0, there was no matching block\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/difflib.py:385\u001b[0m, in \u001b[0;36mSequenceMatcher.find_longest_match\u001b[0;34m(self, alo, ahi, blo, bhi)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m bhi:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m k \u001b[38;5;241m=\u001b[39m newj2len[j] \u001b[38;5;241m=\u001b[39m \u001b[43mj2lenget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m>\u001b[39m bestsize:\n\u001b[1;32m    387\u001b[0m     besti, bestj, bestsize \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m-\u001b[39mk\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, j\u001b[38;5;241m-\u001b[39mk\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, k\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
